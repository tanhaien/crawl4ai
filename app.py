import streamlit as st
import asyncio
from pathlib import Path
from datetime import datetime
import zipfile
import json
import shutil
from urllib.parse import urlparse
from pdf_crawler import PDFCrawler, CONFIG

st.set_page_config(
    page_title="PDF Crawler",
    page_icon="üìÑ",
    layout="wide"
)

def zip_directory(source_dir: Path, output_path: Path):
    """Zip a directory and return the zip file path"""
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for file in source_dir.rglob('*'):
            if file.is_file():
                zipf.write(file, file.relative_to(source_dir))
    return output_path

def main():
    st.title("üìÑ PDF Crawler")
    st.markdown("**Nh·∫≠p URL v√† crawl t·∫•t c·∫£ file PDF t·ª´ website**")
    
    # Initialize session state for crawl results
    if 'crawl_results' not in st.session_state:
        st.session_state.crawl_results = None
    
    # URL input
    st.subheader("Nh·∫≠p URLs")
    urls_input = st.text_area(
        "Nh·∫≠p m·ªôt ho·∫∑c nhi·ªÅu URL (m·ªói d√≤ng m·ªôt URL)",
        height=150,
        placeholder="https://example.com/documents\nhttps://another-site.com/papers"
    )
    
    # Configuration
    col1, col2, col3 = st.columns(3)
    with col1:
        max_pages = st.number_input(
            "S·ªë trang t·ªëi ƒëa m·ªói site",
            min_value=1,
            max_value=200,
            value=50,
            help="Gi·ªõi h·∫°n s·ªë trang web s·∫Ω crawl t·ª´ m·ªói site"
        )
    
    with col2:
        max_concurrent = st.number_input(
            "S·ªë download ƒë·ªìng th·ªùi",
            min_value=1,
            max_value=20,
            value=5,
            help="S·ªë file PDF c√≥ th·ªÉ download c√πng l√∫c"
        )
    
    with col3:
        timeout = st.number_input(
            "Timeout (gi√¢y)",
            min_value=10,
            max_value=180,
            value=60,
            help="Th·ªùi gian ch·ªù t·ªëi ƒëa cho m·ªói request"
        )
    
    # Start button
    if st.button("üöÄ B·∫Øt ƒë·∫ßu Crawl", type="primary", use_container_width=True):
        # Parse URLs
        urls = [url.strip() for url in urls_input.split('\n') if url.strip()]
        
        if not urls:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt URL")
            return
        
        # Validate URLs
        invalid_urls = []
        valid_urls = []
        for url in urls:
            try:
                parsed = urlparse(url)
                if parsed.scheme in ('http', 'https') and parsed.netloc:
                    valid_urls.append(url)
                else:
                    invalid_urls.append(url)
            except Exception:
                invalid_urls.append(url)
        
        if invalid_urls:
            st.error(f"‚ùå URL kh√¥ng h·ª£p l·ªá: {', '.join(invalid_urls)}")
            st.info("‚ÑπÔ∏è URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http:// ho·∫∑c https://")
            return
        
        urls = valid_urls
        
        # Update config
        CONFIG["max_pages_per_site"] = max_pages
        CONFIG["max_concurrent_downloads"] = max_concurrent
        CONFIG["timeout"] = timeout
        
        # Create unique output directory for this run in /tmp for Streamlit Cloud
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        run_dir = Path(f"/tmp/runs/run_{timestamp}")
        output_dir = run_dir / "downloaded_pdfs"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Update CONFIG paths
        CONFIG["output_dir"] = str(output_dir)
        CONFIG["log_file"] = str(run_dir / "pdf_crawler.log")
        CONFIG["metadata_file"] = str(run_dir / "pdf_downloads_metadata.json")
        CONFIG["progress_file"] = str(run_dir / "pdf_crawler_progress.json")
        
        # Progress indicators using session state
        if 'progress_bar' not in st.session_state:
            st.session_state.progress_bar = st.progress(0)
        if 'status_text' not in st.session_state:
            st.session_state.status_text = st.empty()
        
        progress_bar = st.session_state.progress_bar
        status_text = st.session_state.status_text
        
        try:
            status_text.text("üîÑ ƒêang kh·ªüi t·∫°o crawler...")
            crawler = PDFCrawler()
            
            status_text.text(f"üîç ƒêang crawl {len(urls)} site(s)...")
            
            # Run the crawler using existing event loop
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            loop.run_until_complete(crawler.run(urls))
            
            progress_bar.progress(100)
            status_text.text("‚úÖ Ho√†n th√†nh!")
            
            # Load URL mapping from metadata
            url_mapping = {}
            metadata_file = Path(CONFIG["metadata_file"])
            metadata = {}
            if metadata_file.exists():
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                    url_mapping = metadata.get("downloaded_pdfs", {})
            
            # Save all results to session state
            st.session_state.crawl_results = {
                'metadata': crawler.metadata,
                'failed_downloads': crawler.failed_downloads,
                'run_dir': run_dir,
                'output_dir': output_dir,
                'timestamp': timestamp,
                'url_mapping': url_mapping,
                'metadata_file': CONFIG["metadata_file"],
                'log_file': CONFIG["log_file"],
                'full_metadata': metadata
            }
            
            # Trigger display by rerunning
            st.rerun()
                    
        except Exception as e:
            progress_bar.progress(0)
            status_text.text("")
            st.error(f"‚ùå L·ªói: {str(e)}")
            
            # Show log on error
            log_file = Path(CONFIG["log_file"])
            if log_file.exists():
                with st.expander("üìù Chi ti·∫øt l·ªói (log)"):
                    with open(log_file, 'r') as f:
                        st.text(f.read())
    
    # Display results if crawl has been performed
    if st.session_state.crawl_results is not None:
        results = st.session_state.crawl_results
        
        # Display results
        st.success("üéâ Crawl ho√†n t·∫•t!")
        
        # Summary
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Sites ƒë√£ x·ª≠ l√Ω", results['metadata']['sites_processed'])
        with col2:
            st.metric("PDFs t√¨m th·∫•y", results['metadata']['pdfs_found'])
        with col3:
            st.metric("PDFs t·∫£i v·ªÅ", results['metadata']['pdfs_downloaded'])
        with col4:
            st.metric("T·ªïng dung l∆∞·ª£ng", f"{results['metadata']['total_size_mb']:.2f} MB")
        
        # Failed downloads
        if results['failed_downloads']:
            with st.expander(f"‚ö†Ô∏è {len(results['failed_downloads'])} download th·∫•t b·∫°i"):
                for fail in results['failed_downloads'][:10]:  # Show first 10
                    st.text(f"‚Ä¢ {fail['url']}\n  L·ªói: {fail['error']}")
        
        # Metadata display
        with st.expander("üìä Xem metadata chi ti·∫øt"):
            st.json(results['full_metadata'])
        
        # Download section
        if results['metadata']['pdfs_downloaded'] > 0:
            st.subheader("üì• T·∫£i xu·ªëng k·∫øt qu·∫£")

            # File search input
            search_terms = st.text_input(
                "üîç T√¨m ki·∫øm file theo t√™n v√† URL (ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)",
                placeholder="V√≠ d·ª•: catalog, manual, guide...",
                help="T√¨m ki·∫øm trong c·∫£ t√™n file v√† URL g·ªëc. V√≠ d·ª•: 'catalog' s·∫Ω t√¨m c·∫£ file c√≥ t√™n catalog v√† file c√≥ URL ch·ª©a catalog"
            )

            # Get all PDF files
            pdf_files = list(results['output_dir'].rglob("*.pdf"))
            pdf_files.sort()

            # Parse search terms
            search_keywords = [term.strip().lower() for term in search_terms.split(',') if term.strip()] if search_terms else []

            # Separate priority files and other files
            priority_files = []
            other_files = []

            if search_keywords:
                for pdf_file in pdf_files:
                    file_name_lower = pdf_file.name.lower()

                    # Get original URL for this file
                    original_url = ""
                    for url, filepath in results['url_mapping'].items():
                        if Path(filepath).name == pdf_file.name:
                            original_url = url.lower()
                            break

                    # Search in both filename and original URL
                    name_match = any(keyword in file_name_lower for keyword in search_keywords)
                    url_match = any(keyword in original_url for keyword in search_keywords) if original_url else False

                    if name_match or url_match:
                        priority_files.append(pdf_file)
                    else:
                        other_files.append(pdf_file)
            else:
                other_files = pdf_files

            # Multi-select for PDF files
            st.subheader("üìë Ch·ªçn file PDF ƒë·ªÉ t·∫£i xu·ªëng")

            selected_files = []

            # Priority files section
            if priority_files:
                st.markdown("### ‚≠ê File ∆Øu Ti√™n (kh·ªõp t√¨m ki·∫øm)")
                for pdf_file in priority_files:
                    # Get original URL for this file
                    original_url = ""
                    for url, filepath in results['url_mapping'].items():
                        if Path(filepath).name == pdf_file.name:
                            original_url = url
                            break

                    col1, col2 = st.columns([0.05, 0.95])
                    with col1:
                        if st.checkbox("", key=f"priority_{pdf_file}"):
                            selected_files.append(pdf_file)
                    with col2:
                        st.text(f"üéØ {pdf_file.relative_to(results['output_dir'])}")
                        if original_url:
                            st.caption(f"üîó {original_url[:80]}{'...' if len(original_url) > 80 else ''}")

            # Other files section
            if other_files:
                if priority_files:
                    st.markdown("### üìÅ C√°c File Kh√°c")
                else:
                    st.markdown("### üìÅ T·∫•t C·∫£ C√°c File")

                # Show files in batches to avoid UI issues
                batch_size = 20
                for i in range(0, len(other_files), batch_size):
                    batch = other_files[i:i+batch_size]
                    for pdf_file in batch:
                        # Get original URL for this file
                        original_url = ""
                        for url, filepath in results['url_mapping'].items():
                            if Path(filepath).name == pdf_file.name:
                                original_url = url
                                break

                        col1, col2 = st.columns([0.05, 0.95])
                        with col1:
                            if st.checkbox("", key=f"other_{pdf_file}_{i}"):
                                selected_files.append(pdf_file)
                        with col2:
                            st.text(f"üìÑ {pdf_file.relative_to(results['output_dir'])}")
                            if original_url:
                                st.caption(f"üîó {original_url[:80]}{'...' if len(original_url) > 80 else ''}")

            # Summary of selected files
            if selected_files:
                st.success(f"‚úÖ ƒê√£ ch·ªçn {len(selected_files)} file")
            else:
                st.info("‚ÑπÔ∏è Ch∆∞a ch·ªçn file n√†o")

            # Download selected files button
            col1, col2 = st.columns(2)

            with col1:
                if selected_files:
                    # Create zip for selected files
                    selected_zip_path = results['run_dir'] / "selected_pdfs.zip"

                    # Create temporary directory for selected files
                    temp_selected_dir = results['run_dir'] / "temp_selected"
                    temp_selected_dir.mkdir(exist_ok=True)

                    # Copy selected files to temp directory
                    for pdf_file in selected_files:
                        dest_path = temp_selected_dir / pdf_file.name
                        shutil.copy2(pdf_file, dest_path)

                    # Create zip
                    zip_directory(temp_selected_dir, selected_zip_path)

                    # Download button for selected files
                    with open(selected_zip_path, 'rb') as f:
                        st.download_button(
                            label=f"‚¨áÔ∏è T·∫£i {len(selected_files)} file ƒë√£ ch·ªçn",
                            data=f,
                            file_name=f"selected_pdfs_{results['timestamp']}.zip",
                            mime="application/zip",
                            use_container_width=True
                        )

            with col2:
                # Download all files button
                all_zip_path = results['run_dir'] / "all_pdfs.zip"
                zip_directory(results['output_dir'], all_zip_path)

                with open(all_zip_path, 'rb') as f:
                    st.download_button(
                        label=f"‚¨áÔ∏è T·∫£i t·∫•t c·∫£ ({results['metadata']['pdfs_downloaded']} PDFs)",
                        data=f,
                        file_name=f"all_pdfs_{results['timestamp']}.zip",
                        mime="application/zip",
                        use_container_width=True
                    )

            # File statistics
            st.markdown("---")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("T·ªïng s·ªë file", len(pdf_files))
            with col2:
                if search_keywords:
                    st.metric("File ∆Øu ti√™n", len(priority_files))
                else:
                    st.metric("File ∆Øu ti√™n", "0")
            with col3:
                if search_keywords:
                    st.metric("File kh√°c", len(other_files))
                else:
                    st.metric("File kh√°c", len(pdf_files))
        else:
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y PDF n√†o ƒë·ªÉ t·∫£i xu·ªëng")
        
        # Log viewer
        with st.expander("üìù Xem log"):
            log_file = Path(results['log_file'])
            if log_file.exists():
                with open(log_file, 'r') as f:
                    log_content = f.read()
                st.text_area("Log output", log_content, height=300)
        
        # Clear results button
        st.markdown("---")
        if st.button("üîÑ Crawl m·ªõi", key="clear_results", use_container_width=True):
            st.session_state.crawl_results = None
            st.rerun()
    
    # Footer
    st.markdown("---")
    st.markdown("""
    **L∆∞u √Ω:**
    - Crawler s·∫Ω t√¨m v√† t·∫£i t·∫•t c·∫£ file PDF t·ª´ c√°c URL ƒë∆∞·ª£c cung c·∫•p
    - K·∫øt qu·∫£ l∆∞u t·∫°m th·ªùi tr√™n Streamlit Cloud, vui l√≤ng t·∫£i v·ªÅ ngay sau khi crawl xong
    - Th·ªùi gian crawl ph·ª• thu·ªôc v√†o s·ªë l∆∞·ª£ng trang v√† PDF tr√™n website
    """)

if __name__ == "__main__":
    main()
