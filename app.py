import streamlit as st
import asyncio
from pathlib import Path
from datetime import datetime
import zipfile
import json
import shutil
from urllib.parse import urlparse
from pdf_crawler import PDFCrawler, CONFIG

st.set_page_config(
    page_title="PDF Crawler",
    page_icon="üìÑ",
    layout="wide"
)

def zip_directory(source_dir: Path, output_path: Path):
    """Zip a directory and return the zip file path"""
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for file in source_dir.rglob('*'):
            if file.is_file():
                zipf.write(file, file.relative_to(source_dir))
    return output_path

def main():
    st.title("üìÑ PDF Crawler")
    st.markdown("**Nh·∫≠p URL v√† crawl t·∫•t c·∫£ file PDF t·ª´ website**")
    
    # Initialize session state for crawl results
    if 'crawl_results' not in st.session_state:
        st.session_state.crawl_results = None
    if 'discovered_pdfs' not in st.session_state:
        st.session_state.discovered_pdfs = []
    if 'scan_complete' not in st.session_state:
        st.session_state.scan_complete = False
    if 'crawler_instance' not in st.session_state:
        st.session_state.crawler_instance = None
    
    # URL input
    st.subheader("Nh·∫≠p URLs")
    urls_input = st.text_area(
        "Nh·∫≠p m·ªôt ho·∫∑c nhi·ªÅu URL (m·ªói d√≤ng m·ªôt URL)",
        height=150,
        placeholder="https://example.com/documents\nhttps://another-site.com/papers"
    )
    
    # Configuration
    col1, col2, col3 = st.columns(3)
    with col1:
        max_pages = st.number_input(
            "S·ªë trang t·ªëi ƒëa m·ªói site",
            min_value=1,
            max_value=200,
            value=50,
            help="Gi·ªõi h·∫°n s·ªë trang web s·∫Ω crawl t·ª´ m·ªói site"
        )
    
    with col2:
        max_concurrent = st.number_input(
            "S·ªë download ƒë·ªìng th·ªùi",
            min_value=1,
            max_value=20,
            value=5,
            help="S·ªë file PDF c√≥ th·ªÉ download c√πng l√∫c"
        )
    
    with col3:
        timeout = st.number_input(
            "Timeout (gi√¢y)",
            min_value=10,
            max_value=180,
            value=60,
            help="Th·ªùi gian ch·ªù t·ªëi ƒëa cho m·ªói request"
        )
    
    # Phase 1: Discovery Button
    if not st.session_state.scan_complete:
        if st.button("üîç Scan for PDFs (Discovery Phase)", type="primary", use_container_width=True):
            # Parse URLs
            urls = [url.strip() for url in urls_input.split('\n') if url.strip()]
            
            if not urls:
                st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt URL")
                return
            
            # Validate URLs
            invalid_urls = []
            valid_urls = []
            for url in urls:
                try:
                    parsed = urlparse(url)
                    if parsed.scheme in ('http', 'https') and parsed.netloc:
                        valid_urls.append(url)
                    else:
                        invalid_urls.append(url)
                except Exception:
                    invalid_urls.append(url)
            
            if invalid_urls:
                st.error(f"‚ùå URL kh√¥ng h·ª£p l·ªá: {', '.join(invalid_urls)}")
                st.info("‚ÑπÔ∏è URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http:// ho·∫∑c https://")
                return
            
            urls = valid_urls
            
            # Update config
            CONFIG["max_pages_per_site"] = max_pages
            CONFIG["max_concurrent_downloads"] = max_concurrent
            CONFIG["timeout"] = timeout
            
            # Create unique output directory for this run in /tmp for Streamlit Cloud
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            run_dir = Path(f"/tmp/runs/run_{timestamp}")
            output_dir = run_dir / "downloaded_pdfs"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Update CONFIG paths
            CONFIG["output_dir"] = str(output_dir)
            CONFIG["log_file"] = str(run_dir / "pdf_crawler.log")
            CONFIG["metadata_file"] = str(run_dir / "pdf_downloads_metadata.json")
            CONFIG["progress_file"] = str(run_dir / "pdf_crawler_progress.json")
            
            # Progress indicators
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                status_text.text("üîÑ ƒêang kh·ªüi t·∫°o crawler...")
                crawler = PDFCrawler()
                
                status_text.text(f"üîç ƒêang qu√©t {len(urls)} site(s) ƒë·ªÉ t√¨m PDF...")
                
                # Run the crawler in discover mode
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                result = loop.run_until_complete(crawler.run(urls, mode='discover'))
                
                progress_bar.progress(100)
                status_text.text("‚úÖ Qu√©t ho√†n th√†nh!")
                
                # Store discovered PDFs in session state
                st.session_state.discovered_pdfs = crawler.discovered_pdfs
                st.session_state.scan_complete = True
                st.session_state.crawler_instance = crawler
                st.session_state.run_dir = run_dir
                st.session_state.output_dir = output_dir
                st.session_state.timestamp = timestamp
                
                # Trigger display by rerunning
                st.rerun()
                        
            except Exception as e:
                progress_bar.progress(0)
                status_text.text("")
                st.error(f"‚ùå L·ªói: {str(e)}")
                
                # Show log on error
                log_file = Path(CONFIG["log_file"])
                if log_file.exists():
                    with st.expander("üìù Chi ti·∫øt l·ªói (log)"):
                        with open(log_file, 'r') as f:
                            st.text(f.read())
    
    # Phase 2: Display discovered PDFs and Download Selected
    if st.session_state.scan_complete and st.session_state.discovered_pdfs:
        st.success(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(st.session_state.discovered_pdfs)} file PDF!")
        
        st.markdown("---")
        st.subheader("üìã Ch·ªçn PDFs ƒë·ªÉ t·∫£i xu·ªëng")
        
        # Display table header
        col1, col2, col3, col4 = st.columns([0.5, 3, 2, 2])
        with col1:
            st.markdown("**Ch·ªçn**")
        with col2:
            st.markdown("**T√™n file**")
        with col3:
            st.markdown("**Domain**")
        with col4:
            st.markdown("**URL**")
        
        st.markdown("---")
        
        # Display PDFs with checkboxes
        selected_pdfs = []
        for idx, pdf in enumerate(st.session_state.discovered_pdfs):
            col1, col2, col3, col4 = st.columns([0.5, 3, 2, 2])
            with col1:
                selected = st.checkbox("", key=f"pdf_{idx}", label_visibility="hidden")
            with col2:
                st.write(pdf['filename'])
            with col3:
                st.write(pdf['domain'])
            with col4:
                url_display = pdf['url'][:40] + "..." if len(pdf['url']) > 40 else pdf['url']
                st.write(url_display)
            
            if selected:
                selected_pdfs.append(pdf)
        
        st.markdown("---")
        
        # Summary of selection
        if selected_pdfs:
            st.info(f"‚ÑπÔ∏è ƒê√£ ch·ªçn {len(selected_pdfs)} / {len(st.session_state.discovered_pdfs)} file PDF")
        else:
            st.warning("‚ö†Ô∏è Ch∆∞a ch·ªçn file n√†o")
        
        # Download selected PDFs button
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üì• Download Selected PDFs", type="primary", use_container_width=True, disabled=len(selected_pdfs)==0):
                # Progress indicators
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    status_text.text(f"üîÑ ƒêang t·∫£i {len(selected_pdfs)} file PDF...")
                    
                    crawler = st.session_state.crawler_instance
                    
                    # Run download for selected PDFs
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    result = loop.run_until_complete(crawler.download_selected_pdfs(selected_pdfs))
                    
                    progress_bar.progress(100)
                    status_text.text("‚úÖ T·∫£i xu·ªëng ho√†n th√†nh!")
                    
                    # Load URL mapping from metadata
                    url_mapping = {}
                    metadata_file = Path(CONFIG["metadata_file"])
                    metadata = {}
                    if metadata_file.exists():
                        with open(metadata_file, 'r') as f:
                            metadata = json.load(f)
                            url_mapping = metadata.get("downloaded_pdfs", {})
                    
                    # Save all results to session state
                    st.session_state.crawl_results = {
                        'metadata': crawler.metadata,
                        'failed_downloads': crawler.failed_downloads,
                        'run_dir': st.session_state.run_dir,
                        'output_dir': st.session_state.output_dir,
                        'timestamp': st.session_state.timestamp,
                        'url_mapping': url_mapping,
                        'metadata_file': CONFIG["metadata_file"],
                        'log_file': CONFIG["log_file"],
                        'full_metadata': metadata
                    }
                    
                    # Reset discovery state
                    st.session_state.scan_complete = False
                    st.session_state.discovered_pdfs = []
                    
                    # Trigger display by rerunning
                    st.rerun()
                            
                except Exception as e:
                    progress_bar.progress(0)
                    status_text.text("")
                    st.error(f"‚ùå L·ªói: {str(e)}")
                    
                    # Show log on error
                    log_file = Path(CONFIG["log_file"])
                    if log_file.exists():
                        with st.expander("üìù Chi ti·∫øt l·ªói (log)"):
                            with open(log_file, 'r') as f:
                                st.text(f.read())
        
        with col2:
            if st.button("üîÑ Qu√©t l·∫°i", use_container_width=True):
                st.session_state.scan_complete = False
                st.session_state.discovered_pdfs = []
                st.session_state.crawler_instance = None
                st.rerun()
    
    # Display results if crawl has been performed
    if st.session_state.crawl_results is not None:
        results = st.session_state.crawl_results
        
        # Display results
        st.success("üéâ Crawl ho√†n t·∫•t!")
        
        # Summary
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Sites ƒë√£ x·ª≠ l√Ω", results['metadata']['sites_processed'])
        with col2:
            st.metric("PDFs t√¨m th·∫•y", results['metadata']['pdfs_found'])
        with col3:
            st.metric("PDFs t·∫£i v·ªÅ", results['metadata']['pdfs_downloaded'])
        with col4:
            st.metric("T·ªïng dung l∆∞·ª£ng", f"{results['metadata']['total_size_mb']:.2f} MB")
        
        # Failed downloads
        if results['failed_downloads']:
            with st.expander(f"‚ö†Ô∏è {len(results['failed_downloads'])} download th·∫•t b·∫°i"):
                for fail in results['failed_downloads'][:10]:  # Show first 10
                    st.text(f"‚Ä¢ {fail['url']}\n  L·ªói: {fail['error']}")
        
        # Metadata display
        with st.expander("üìä Xem metadata chi ti·∫øt"):
            st.json(results['full_metadata'])
        
        # Download section
        if results['metadata']['pdfs_downloaded'] > 0:
            st.subheader("üì• T·∫£i xu·ªëng k·∫øt qu·∫£")

            # File search input
            search_terms = st.text_input(
                "üîç T√¨m ki·∫øm file theo t√™n v√† URL (ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)",
                placeholder="V√≠ d·ª•: catalog, manual, guide...",
                help="T√¨m ki·∫øm trong c·∫£ t√™n file v√† URL g·ªëc. V√≠ d·ª•: 'catalog' s·∫Ω t√¨m c·∫£ file c√≥ t√™n catalog v√† file c√≥ URL ch·ª©a catalog"
            )

            # Get all PDF files
            pdf_files = list(results['output_dir'].rglob("*.pdf"))
            pdf_files.sort()

            # Parse search terms
            search_keywords = [term.strip().lower() for term in search_terms.split(',') if term.strip()] if search_terms else []

            # Separate priority files and other files
            priority_files = []
            other_files = []

            if search_keywords:
                for pdf_file in pdf_files:
                    file_name_lower = pdf_file.name.lower()

                    # Get original URL for this file
                    original_url = ""
                    for url, filepath in results['url_mapping'].items():
                        if Path(filepath).name == pdf_file.name:
                            original_url = url.lower()
                            break

                    # Search in both filename and original URL
                    name_match = any(keyword in file_name_lower for keyword in search_keywords)
                    url_match = any(keyword in original_url for keyword in search_keywords) if original_url else False

                    if name_match or url_match:
                        priority_files.append(pdf_file)
                    else:
                        other_files.append(pdf_file)
            else:
                other_files = pdf_files

            # Multi-select for PDF files
            st.subheader("üìë Ch·ªçn file PDF ƒë·ªÉ t·∫£i xu·ªëng")

            selected_files = []

            # Priority files section
            if priority_files:
                st.markdown("### ‚≠ê File ∆Øu Ti√™n (kh·ªõp t√¨m ki·∫øm)")
                for pdf_file in priority_files:
                    # Get original URL for this file
                    original_url = ""
                    for url, filepath in results['url_mapping'].items():
                        if Path(filepath).name == pdf_file.name:
                            original_url = url
                            break

                    col1, col2 = st.columns([0.05, 0.95])
                    with col1:
                        if st.checkbox("Select file", key=f"priority_{pdf_file}", label_visibility="hidden"):
                            selected_files.append(pdf_file)
                    with col2:
                        st.text(f"üéØ {pdf_file.relative_to(results['output_dir'])}")
                        if original_url:
                            st.caption(f"üîó {original_url[:80]}{'...' if len(original_url) > 80 else ''}")

            # Other files section
            if other_files:
                if priority_files:
                    st.markdown("### üìÅ C√°c File Kh√°c")
                else:
                    st.markdown("### üìÅ T·∫•t C·∫£ C√°c File")

                # Show files in batches to avoid UI issues
                batch_size = 20
                for i in range(0, len(other_files), batch_size):
                    batch = other_files[i:i+batch_size]
                    for pdf_file in batch:
                        # Get original URL for this file
                        original_url = ""
                        for url, filepath in results['url_mapping'].items():
                            if Path(filepath).name == pdf_file.name:
                                original_url = url
                                break

                        col1, col2 = st.columns([0.05, 0.95])
                        with col1:
                            if st.checkbox("Select file", key=f"other_{pdf_file}_{i}", label_visibility="hidden"):
                                selected_files.append(pdf_file)
                        with col2:
                            st.text(f"üìÑ {pdf_file.relative_to(results['output_dir'])}")
                            if original_url:
                                st.caption(f"üîó {original_url[:80]}{'...' if len(original_url) > 80 else ''}")

            # Summary of selected files
            if selected_files:
                st.success(f"‚úÖ ƒê√£ ch·ªçn {len(selected_files)} file")
            else:
                st.info("‚ÑπÔ∏è Ch∆∞a ch·ªçn file n√†o")

            # Initialize session state for download preparation
            if 'prepare_download' not in st.session_state:
                st.session_state.prepare_download = False

            # Button to prepare downloads
            st.markdown("---")
            if st.button("üì• Download c√°c file ƒë√£ ch·ªçn", type="primary", use_container_width=True, disabled=len(selected_files) == 0):
                st.session_state.prepare_download = True
                st.rerun()

            # Show download buttons only after user clicks "Prepare Download"
            if st.session_state.prepare_download:
                st.markdown("### üì¶ T·∫£i xu·ªëng")
                
                col1, col2 = st.columns(2)

                with col1:
                    if selected_files:
                        # Create zip for selected files
                        selected_zip_path = results['run_dir'] / "selected_pdfs.zip"

                        # Create temporary directory for selected files
                        temp_selected_dir = results['run_dir'] / "temp_selected"
                        temp_selected_dir.mkdir(exist_ok=True)

                        # Copy selected files to temp directory
                        for pdf_file in selected_files:
                            dest_path = temp_selected_dir / pdf_file.name
                            shutil.copy2(pdf_file, dest_path)

                        # Create zip
                        zip_directory(temp_selected_dir, selected_zip_path)

                        # Download button for selected files
                        with open(selected_zip_path, 'rb') as f:
                            st.download_button(
                                label=f"‚¨áÔ∏è T·∫£i {len(selected_files)} file ƒë√£ ch·ªçn",
                                data=f,
                                file_name=f"selected_pdfs_{results['timestamp']}.zip",
                                mime="application/zip",
                                use_container_width=True
                            )

                with col2:
                    # Download all files button
                    all_zip_path = results['run_dir'] / "all_pdfs.zip"
                    zip_directory(results['output_dir'], all_zip_path)

                    with open(all_zip_path, 'rb') as f:
                        st.download_button(
                            label=f"‚¨áÔ∏è T·∫£i t·∫•t c·∫£ ({results['metadata']['pdfs_downloaded']} PDFs)",
                            data=f,
                            file_name=f"all_pdfs_{results['timestamp']}.zip",
                            mime="application/zip",
                            use_container_width=True
                        )
                
                # Reset button
                if st.button("üîÑ Ch·ªçn l·∫°i file", use_container_width=True):
                    st.session_state.prepare_download = False
                    st.rerun()

            # File statistics
            st.markdown("---")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("T·ªïng s·ªë file", len(pdf_files))
            with col2:
                if search_keywords:
                    st.metric("File ∆Øu ti√™n", len(priority_files))
                else:
                    st.metric("File ∆Øu ti√™n", "0")
            with col3:
                if search_keywords:
                    st.metric("File kh√°c", len(other_files))
                else:
                    st.metric("File kh√°c", len(pdf_files))
        else:
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y PDF n√†o ƒë·ªÉ t·∫£i xu·ªëng")
        
        # Log viewer
        with st.expander("üìù Xem log"):
            log_file = Path(results['log_file'])
            if log_file.exists():
                with open(log_file, 'r') as f:
                    log_content = f.read()
                st.text_area("Log output", log_content, height=300)
        
        # Clear results button
        st.markdown("---")
        if st.button("üîÑ Crawl m·ªõi", key="clear_results", use_container_width=True):
            st.session_state.crawl_results = None
            st.rerun()
    
    # Footer
    st.markdown("---")
    st.markdown("""
    **L∆∞u √Ω:**
    - Crawler s·∫Ω t√¨m v√† t·∫£i t·∫•t c·∫£ file PDF t·ª´ c√°c URL ƒë∆∞·ª£c cung c·∫•p
    - K·∫øt qu·∫£ l∆∞u t·∫°m th·ªùi tr√™n Streamlit Cloud, vui l√≤ng t·∫£i v·ªÅ ngay sau khi crawl xong
    - Th·ªùi gian crawl ph·ª• thu·ªôc v√†o s·ªë l∆∞·ª£ng trang v√† PDF tr√™n website
    """)

if __name__ == "__main__":
    main()
