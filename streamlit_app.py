import streamlit as st
import asyncio
from pathlib import Path
from datetime import datetime
import zipfile
import json
import shutil
from pdf_crawler import PDFCrawler, CONFIG

st.set_page_config(
    page_title="PDF Crawler",
    page_icon="üìÑ",
    layout="wide"
)

def zip_directory(source_dir: Path, output_path: Path):
    """Zip a directory and return the zip file path"""
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for file in source_dir.rglob('*'):
            if file.is_file():
                zipf.write(file, file.relative_to(source_dir))
    return output_path

def main():
    st.title("üìÑ PDF Crawler")
    st.markdown("**Nh·∫≠p URL v√† crawl t·∫•t c·∫£ file PDF t·ª´ website**")
    
    # URL input
    st.subheader("Nh·∫≠p URLs")
    urls_input = st.text_area(
        "Nh·∫≠p m·ªôt ho·∫∑c nhi·ªÅu URL (m·ªói d√≤ng m·ªôt URL)",
        height=150,
        placeholder="https://example.com/documents\nhttps://another-site.com/papers"
    )
    
    # Configuration
    col1, col2, col3 = st.columns(3)
    with col1:
        max_pages = st.number_input(
            "S·ªë trang t·ªëi ƒëa m·ªói site",
            min_value=1,
            max_value=200,
            value=50,
            help="Gi·ªõi h·∫°n s·ªë trang web s·∫Ω crawl t·ª´ m·ªói site"
        )
    
    with col2:
        max_concurrent = st.number_input(
            "S·ªë download ƒë·ªìng th·ªùi",
            min_value=1,
            max_value=20,
            value=5,
            help="S·ªë file PDF c√≥ th·ªÉ download c√πng l√∫c"
        )
    
    with col3:
        timeout = st.number_input(
            "Timeout (gi√¢y)",
            min_value=10,
            max_value=180,
            value=60,
            help="Th·ªùi gian ch·ªù t·ªëi ƒëa cho m·ªói request"
        )
    
    # Start button
    if st.button("üöÄ B·∫Øt ƒë·∫ßu Crawl", type="primary", use_container_width=True):
        # Parse URLs
        urls = [url.strip() for url in urls_input.split('\n') if url.strip()]
        
        if not urls:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt URL")
            return
        
        # Update config
        CONFIG["max_pages_per_site"] = max_pages
        CONFIG["max_concurrent_downloads"] = max_concurrent
        CONFIG["timeout"] = timeout
        
        # Create unique output directory for this run
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        run_dir = Path(f"runs/run_{timestamp}")
        output_dir = run_dir / "downloaded_pdfs"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Update CONFIG paths
        CONFIG["output_dir"] = str(output_dir)
        CONFIG["log_file"] = str(run_dir / "pdf_crawler.log")
        CONFIG["metadata_file"] = str(run_dir / "pdf_downloads_metadata.json")
        CONFIG["progress_file"] = str(run_dir / "pdf_crawler_progress.json")
        
        # Progress indicators
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            status_text.text("üîÑ ƒêang kh·ªüi t·∫°o crawler...")
            crawler = PDFCrawler()
            
            status_text.text(f"üîç ƒêang crawl {len(urls)} site(s)...")
            
            # Run the crawler
            async def run_crawler():
                await crawler.run(urls)
            
            asyncio.run(run_crawler())
            
            progress_bar.progress(100)
            status_text.text("‚úÖ Ho√†n th√†nh!")
            
            # Display results
            st.success("üéâ Crawl ho√†n t·∫•t!")
            
            # Summary
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Sites ƒë√£ x·ª≠ l√Ω", crawler.metadata['sites_processed'])
            with col2:
                st.metric("PDFs t√¨m th·∫•y", crawler.metadata['pdfs_found'])
            with col3:
                st.metric("PDFs t·∫£i v·ªÅ", crawler.metadata['pdfs_downloaded'])
            with col4:
                st.metric("T·ªïng dung l∆∞·ª£ng", f"{crawler.metadata['total_size_mb']:.2f} MB")
            
            # Failed downloads
            if crawler.failed_downloads:
                with st.expander(f"‚ö†Ô∏è {len(crawler.failed_downloads)} download th·∫•t b·∫°i"):
                    for fail in crawler.failed_downloads[:10]:  # Show first 10
                        st.text(f"‚Ä¢ {fail['url']}\n  L·ªói: {fail['error']}")
            
            # Metadata display
            with st.expander("üìä Xem metadata chi ti·∫øt"):
                metadata_file = Path(CONFIG["metadata_file"])
                if metadata_file.exists():
                    with open(metadata_file, 'r') as f:
                        metadata = json.load(f)
                    st.json(metadata)
            
            # Download section
            if crawler.metadata['pdfs_downloaded'] > 0:
                st.subheader("üì• T·∫£i xu·ªëng k·∫øt qu·∫£")
                
                # Create zip file
                zip_path = run_dir / "pdfs.zip"
                status_text.text("üì¶ ƒêang ƒë√≥ng g√≥i files...")
                zip_directory(output_dir, zip_path)
                
                # Download button for zip
                with open(zip_path, 'rb') as f:
                    st.download_button(
                        label=f"‚¨áÔ∏è T·∫£i t·∫•t c·∫£ ({crawler.metadata['pdfs_downloaded']} PDFs)",
                        data=f,
                        file_name=f"pdfs_{timestamp}.zip",
                        mime="application/zip",
                        use_container_width=True
                    )
                
                # Show list of downloaded files
                with st.expander("üìë Danh s√°ch file ƒë√£ t·∫£i"):
                    pdf_files = list(output_dir.rglob("*.pdf"))
                    for i, pdf_file in enumerate(pdf_files[:50], 1):  # Show first 50
                        st.text(f"{i}. {pdf_file.relative_to(output_dir)}")
                    if len(pdf_files) > 50:
                        st.text(f"... v√† {len(pdf_files) - 50} file kh√°c")
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y PDF n√†o ƒë·ªÉ t·∫£i xu·ªëng")
            
            # Log viewer
            with st.expander("üìù Xem log"):
                log_file = Path(CONFIG["log_file"])
                if log_file.exists():
                    with open(log_file, 'r') as f:
                        log_content = f.read()
                    st.text_area("Log output", log_content, height=300)
                    
        except Exception as e:
            progress_bar.progress(0)
            status_text.text("")
            st.error(f"‚ùå L·ªói: {str(e)}")
            
            # Show log on error
            log_file = Path(CONFIG["log_file"])
            if log_file.exists():
                with st.expander("üìù Chi ti·∫øt l·ªói (log)"):
                    with open(log_file, 'r') as f:
                        st.text(f.read())
    
    # Footer
    st.markdown("---")
    st.markdown("""
    **L∆∞u √Ω:**
    - Crawler s·∫Ω t√¨m v√† t·∫£i t·∫•t c·∫£ file PDF t·ª´ c√°c URL ƒë∆∞·ª£c cung c·∫•p
    - K·∫øt qu·∫£ l∆∞u t·∫°m th·ªùi tr√™n Streamlit Cloud, vui l√≤ng t·∫£i v·ªÅ ngay sau khi crawl xong
    - Th·ªùi gian crawl ph·ª• thu·ªôc v√†o s·ªë l∆∞·ª£ng trang v√† PDF tr√™n website
    """)

if __name__ == "__main__":
    main()
